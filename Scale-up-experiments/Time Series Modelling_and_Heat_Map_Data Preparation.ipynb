{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "006fda19-805d-49a7-bb18-8013adf20180",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e33fc5aa-921f-4fe4-a2a8-528421f18039",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(sc, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a184783-3aa6-46bf-929c-ae8d42324b44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"MLlib lab\") \\\n",
    "    .config(\"spark.sql.parquet.enableVectorizedReader\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "# swith the latest spark version to older one so that it tolerates some data format issues\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "\"\"\" \n",
    "in order to avoid \"Parquet column cannot be converted\" error, we need to disable vectorized reader when we have decimal values in our columns. \n",
    "refer to https://learn.microsoft.com/en-us/answers/questions/853861/parquet-column-cannot-be-converted for further info\n",
    "\"\"\"\n",
    "# spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\", \"false\") \n",
    "\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2b6bf1d-592f-4b2a-9df9-91b6c68457c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec98d14f-57ea-41cb-ba97-9ce367e68ef1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampNTZType, LongType, DoubleType,IntegerType\n",
    "\"\"\"\n",
    "We have encountered \"Parquet column cannot be converted\" error. As a workaround we decided to loop through directory and ensure there is no column type mismatch by checking file by file.\n",
    "\"\"\"\n",
    "\n",
    "directory = '/mnt/2024-team14/'\n",
    "\n",
    "# List all parquet files\n",
    "parquet_files = dbutils.fs.ls(directory)\n",
    "#print(parquet_files)\n",
    "#weather_file = parquet_files.pop()\n",
    "#taxi_lookup = parquet_files.pop()\n",
    "#json = parquet_files.pop()\n",
    "#traffic = parquet_files.pop()\n",
    "\n",
    "# Loop through directory and check where there is a column type mismatch between files\n",
    "\"\"\"prev_types, curr_types = None, None\n",
    "mismatches = {}\n",
    "for file in parquet_files: \n",
    "  if file.path == \"/mnt/2024-team14/weather_data.csv\": \n",
    "    df1 = spark.read.csv(path) \n",
    "    continue\n",
    "  # Read the Parquet file with schema inference\n",
    "  df = spark.read.parquet(file.path)\n",
    "\n",
    "  if not prev_types and not curr_types:\n",
    "    curr_types = df.dtypes\n",
    "    continue\n",
    "  \n",
    "  prev_types = curr_types\n",
    "  curr_types = df.dtypes\n",
    "\n",
    "  # check each column\n",
    "  for i in range(len(df.columns)):\n",
    "    if prev_types[i] != curr_types[i]:\n",
    "      if not file.name in mismatches:\n",
    "        mismatches[file.name] = [(prev_types[i][0], (prev_types[i][1], curr_types[i][1]))]\n",
    "      else:\n",
    "        mismatches[file.name].append((prev_types[i][0], (prev_types[i][1], curr_types[i][1])))\"\"\"\n",
    "\n",
    "# print(mismatches)\n",
    "schema = StructType([\n",
    "  StructField('hvfhs_license_num', StringType(), nullable=True), \n",
    "  StructField('dispatching_base_num', StringType(), nullable=True), \n",
    "  StructField('originating_base_num', StringType(), nullable=True), \n",
    "  StructField('request_datetime', TimestampNTZType(), nullable=True), \n",
    "  StructField('on_scene_datetime', TimestampNTZType(), nullable=True), \n",
    "  StructField('pickup_datetime', TimestampNTZType(), nullable=True), \n",
    "  StructField('dropoff_datetime', TimestampNTZType(), nullable=True), \n",
    "  StructField('PULocationID', LongType(), nullable=True), \n",
    "  StructField('DOLocationID', LongType(), nullable=True), \n",
    "  StructField('trip_miles', DoubleType(), nullable=True), \n",
    "  StructField('trip_time', LongType(), nullable=True), \n",
    "  StructField('base_passenger_fare', DoubleType(), nullable=True), \n",
    "  StructField('tolls', DoubleType(), nullable=True), \n",
    "  StructField('bcf', DoubleType(), nullable=True), \n",
    "  StructField('sales_tax', DoubleType(), nullable=True), \n",
    "  StructField('congestion_surcharge', DoubleType(), nullable=True), \n",
    "  StructField('airport_fee', IntegerType(), nullable=True), \n",
    "  StructField('tips', DoubleType(), nullable=True), \n",
    "  StructField('driver_pay', DoubleType(), nullable=True), \n",
    "  StructField('shared_request_flag', StringType(), nullable=True), \n",
    "  StructField('shared_match_flag', StringType(), nullable=True), \n",
    "  StructField('access_a_ride_flag', StringType(), nullable=True), \n",
    "  StructField('wav_request_flag', StringType(), nullable=True), \n",
    "  StructField('wav_match_flag', IntegerType(), nullable=True)\n",
    "])\n",
    "# Create empty dataframe\n",
    "union_df = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)\n",
    "\n",
    "# Process each Parquet file\n",
    "for file in parquet_files:\n",
    "  if file.path == \"dbfs:/mnt/2024-team14/MTA_2020mar_2024apr.csv\":\n",
    "    df3 = spark.read.csv(file.path, header=True)\n",
    "  if file.path == \"dbfs:/mnt/2024-team14/weather_data.csv\":\n",
    "    df1 = spark.read.csv(file.path, header=True)\n",
    "  if file.path == 'dbfs:/mnt/2024-team14/taxi_zone_lookup.csv':\n",
    "    df2 = spark.read.csv(file.path, header=True)\n",
    "  if 'parquet' in file.path:  \n",
    "    df = spark.read.parquet(file.path)\n",
    "    # Read the Parquet file with schema inference\n",
    "  #df = spark.read.parquet(file.path)\n",
    "\n",
    "    # (column with mismatch, desirable type)\n",
    "  mismatch_col = [\n",
    "      (\"wav_match_flag\", \"string\"), \n",
    "      (\"airport_fee\", \"double\"), \n",
    "      (\"PULocationID\", \"bigint\"), \n",
    "      (\"DOLocationID\", \"bigint\")\n",
    "    ]\n",
    "\n",
    "  df = df.withColumns({c: F.col(c).cast(t) for c, t in mismatch_col})\n",
    "\n",
    "    # Union the casted DataFrame with the union_df\n",
    "  union_df = union_df.union(df)\n",
    "\n",
    "# Revert the variable name after unioning the whole data\n",
    "df = union_df\n",
    "\n",
    "total_rows = df.count()\n",
    "print(f\"Total number of rows:{total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43e0bd00-e175-4bab-ba9f-8e5f41d756b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating new columns for date, day, week, month and year\n",
    "df=df.withColumn(\"day\",F.dayofyear('pickup_datetime'))\n",
    "df=df.withColumn(\"hour\",F.hour('pickup_datetime'))\n",
    "df=df.withColumn(\"month\",F.month('pickup_datetime'))\n",
    "df=df.withColumn(\"year\",F.year('pickup_datetime'))\n",
    "df=df.withColumn(\"Date\",F.to_date('pickup_datetime'))\n",
    "df=df.withColumn(\"week\",F.weekofyear('pickup_datetime'))\n",
    "df=df.withColumn(\"week\",F.when(F.col(\"year\") == 2019, F.col(\"week\")).when(F.col(\"year\") == 2020, F.col(\"week\") + 48)\n",
    "  .when(F.col(\"year\") == 2021, F.col(\"week\")+ 48 + 52).when(F.col(\"year\") == 2022, F.col(\"week\") + 48 + 52 + 52).when(F.col(\"year\")==2023,F.col(\"week\")+ 48 + 52 + 52 + 52).otherwise(F.col(\"week\")+ 48 + 52 + 52 + 52 + 52))\n",
    "df = df.withColumn(\"speed\",F.col(\"trip_miles\") / F.col(\"trip_time\") * 3600)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f08056c-95ec-480a-a68f-31ff5efc9726",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import holidays\n",
    "year_range = (2019, 2025)\n",
    "hds = []\n",
    "for y in range(year_range[0], year_range[1]):\n",
    "  hds += holidays.US(state=\"NY\", years=y).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cecd0d5-ed6a-486b-8d20-4b9fe954b406",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumns({\n",
    "  \"isWeekend\": F.when(F.col(\"Date\").isin(hds) | F.dayofweek(F.col(\"Date\")).isin([1, 7]), 1).otherwise(0),\n",
    "  \"isOvernight\": F.when(F.col(\"hour\").isin(list(range(20, 24))+list(range(0, 6))), 1).otherwise(0)}) \\\n",
    "  .withColumn(\"isRushhour\", \n",
    "              F.when(F.col(\"hour\").isin(list(range(16, 20))) | (F.col(\"isWeekend\") == 0), 1).otherwise(0)\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2be6be9e-5009-4193-a32c-5593b8a78aba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_time = df.groupBy(\"week\").count()\n",
    "abc = df_time.select(\"week\",\"count\").orderBy(\"week\").collect()\n",
    "(x_values, y_values) = zip(*abc)\n",
    "plt.plot(x_values, y_values)\n",
    "plt.title('Time Series')\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('Number of Trips')\n",
    "plt.show()\n",
    "display(df_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "67ba4d3d-aa55-457b-ad22-f223d7ac73fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5515f59-daa5-424c-9021-a943edd2726d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pu_zone = df.groupBy(\"PUZone\").count()\n",
    "display(pu_zone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fbdb8a4-c973-4ac1-9e24-efae36eb2895",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "do_zone = df.groupBy(\"DOZone\").count()\n",
    "display(do_zone)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Time Series Modelling_and_Heat_Map_Data Preparation",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
