{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "264a6e77-5848-4476-8933-9efd826184b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b56a09d4-df75-497b-85ff-3300d1d9e09e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(sc, spark) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34480a76-3a29-432f-810d-22fff2177806",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"MLlib lab\") \\\n",
    "    .config(\"spark.sql.parquet.enableVectorizedReader\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "# swith the latest spark version to older one so that it tolerates some data format issues\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "\"\"\" \n",
    "in order to avoid \"Parquet column cannot be converted\" error, we need to disable vectorized reader when we have decimal values in our columns. \n",
    "refer to https://learn.microsoft.com/en-us/answers/questions/853861/parquet-column-cannot-be-converted for further info\n",
    "\"\"\"\n",
    "# spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\", \"false\") \n",
    "\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f27153c-da02-433b-ad0d-94c01ee599d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fc4c7b7-070d-4c67-9089-65cf1e5ad55e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampNTZType, LongType, DoubleType,IntegerType\n",
    "\"\"\"\n",
    "We have encountered \"Parquet column cannot be converted\" error. As a workaround we decided to loop through directory and ensure there is no column type mismatch by checking file by file.\n",
    "\"\"\"\n",
    "\n",
    "directory = '/mnt/2024-team14/'\n",
    "\n",
    "# List all parquet files\n",
    "parquet_files = dbutils.fs.ls(directory)\n",
    "#print(parquet_files)\n",
    "#weather_file = parquet_files.pop()\n",
    "#taxi_lookup = parquet_files.pop()\n",
    "#json = parquet_files.pop()\n",
    "#traffic = parquet_files.pop()\n",
    "\n",
    "# Loop through directory and check where there is a column type mismatch between files\n",
    "\"\"\"prev_types, curr_types = None, None\n",
    "mismatches = {}\n",
    "for file in parquet_files: \n",
    "  if file.path == \"/mnt/2024-team14/weather_data.csv\": \n",
    "    df1 = spark.read.csv(path) \n",
    "    continue\n",
    "  # Read the Parquet file with schema inference\n",
    "  df = spark.read.parquet(file.path)\n",
    "\n",
    "  if not prev_types and not curr_types:\n",
    "    curr_types = df.dtypes\n",
    "    continue\n",
    "  \n",
    "  prev_types = curr_types\n",
    "  curr_types = df.dtypes\n",
    "\n",
    "  # check each column\n",
    "  for i in range(len(df.columns)):\n",
    "    if prev_types[i] != curr_types[i]:\n",
    "      if not file.name in mismatches:\n",
    "        mismatches[file.name] = [(prev_types[i][0], (prev_types[i][1], curr_types[i][1]))]\n",
    "      else:\n",
    "        mismatches[file.name].append((prev_types[i][0], (prev_types[i][1], curr_types[i][1])))\"\"\"\n",
    "\n",
    "# print(mismatches)\n",
    "schema = StructType([\n",
    "  StructField('hvfhs_license_num', StringType(), nullable=True), \n",
    "  StructField('dispatching_base_num', StringType(), nullable=True), \n",
    "  StructField('originating_base_num', StringType(), nullable=True), \n",
    "  StructField('request_datetime', TimestampNTZType(), nullable=True), \n",
    "  StructField('on_scene_datetime', TimestampNTZType(), nullable=True), \n",
    "  StructField('pickup_datetime', TimestampNTZType(), nullable=True), \n",
    "  StructField('dropoff_datetime', TimestampNTZType(), nullable=True), \n",
    "  StructField('PULocationID', LongType(), nullable=True), \n",
    "  StructField('DOLocationID', LongType(), nullable=True), \n",
    "  StructField('trip_miles', DoubleType(), nullable=True), \n",
    "  StructField('trip_time', LongType(), nullable=True), \n",
    "  StructField('base_passenger_fare', DoubleType(), nullable=True), \n",
    "  StructField('tolls', DoubleType(), nullable=True), \n",
    "  StructField('bcf', DoubleType(), nullable=True), \n",
    "  StructField('sales_tax', DoubleType(), nullable=True), \n",
    "  StructField('congestion_surcharge', DoubleType(), nullable=True), \n",
    "  StructField('airport_fee', IntegerType(), nullable=True), \n",
    "  StructField('tips', DoubleType(), nullable=True), \n",
    "  StructField('driver_pay', DoubleType(), nullable=True), \n",
    "  StructField('shared_request_flag', StringType(), nullable=True), \n",
    "  StructField('shared_match_flag', StringType(), nullable=True), \n",
    "  StructField('access_a_ride_flag', StringType(), nullable=True), \n",
    "  StructField('wav_request_flag', StringType(), nullable=True), \n",
    "  StructField('wav_match_flag', IntegerType(), nullable=True)\n",
    "])\n",
    "# Create empty dataframe\n",
    "union_df = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)\n",
    "\n",
    "# Process each Parquet file\n",
    "for file in parquet_files:\n",
    "  if file.path == \"dbfs:/mnt/2024-team14/MTA_2020mar_2024apr.csv\":\n",
    "    df3 = spark.read.csv(file.path, header=True)\n",
    "  if file.path == \"dbfs:/mnt/2024-team14/weather_data.csv\":\n",
    "    df1 = spark.read.csv(file.path, header=True)\n",
    "  if file.path == 'dbfs:/mnt/2024-team14/taxi_zone_lookup.csv':\n",
    "    df2 = spark.read.csv(file.path, header=True)\n",
    "  if \"parquet\" in file.path:  \n",
    "    df = spark.read.parquet(file.path)\n",
    "    # Read the Parquet file with schema inference\n",
    "  #df = spark.read.parquet(file.path)\n",
    "\n",
    "    # (column with mismatch, desirable type)\n",
    "  mismatch_col = [\n",
    "      (\"wav_match_flag\", \"string\"), \n",
    "      (\"airport_fee\", \"double\"), \n",
    "      (\"PULocationID\", \"bigint\"), \n",
    "      (\"DOLocationID\", \"bigint\")\n",
    "    ]\n",
    "\n",
    "  df = df.withColumns({c: F.col(c).cast(t) for c, t in mismatch_col})\n",
    "\n",
    "    # Union the casted DataFrame with the union_df\n",
    "  union_df = union_df.union(df)\n",
    "\n",
    "# Revert the variable name after unioning the whole data\n",
    "df = union_df\n",
    "\n",
    "total_rows = df.count()\n",
    "print(f\"Total number of rows:{total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38f1be6c-a435-4a15-927a-e1498343699d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=df.withColumn(\"day\",F.dayofyear('pickup_datetime'))\n",
    "df=df.withColumn(\"hour\",F.hour('pickup_datetime'))\n",
    "#df=df.withColumn(\"month\",F.month('pickup_datetime'))\n",
    "df=df.withColumn(\"year\",F.year('pickup_datetime'))\n",
    "df=df.withColumn(\"Date\",F.to_date('pickup_datetime'))\n",
    "drop = ['hvfhs_license_num', 'dispatching_base_num', 'originating_base_num', 'request_datetime', 'on_scene_datetime', 'pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID', 'tolls', 'bcf', 'sales_tax', 'congestion_surcharge',\n",
    " 'airport_fee', 'tips', 'driver_pay', 'shared_request_flag', 'shared_match_flag', 'access_a_ride_flag', 'wav_request_flag', 'wav_match_flag']\n",
    "df = df.drop(*drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78f03f48-9850-4634-9b29-6aed88a70d71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import holidays\n",
    "year_range = (2019, 2025)\n",
    "hds = []\n",
    "for y in range(year_range[0], year_range[1]):\n",
    "  hds += holidays.US(state=\"NY\", years=y).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4485c6c5-fb3a-4689-beeb-6f05cfb97a11",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumns({\n",
    "  \"isWeekend\": F.when(F.col(\"Date\").isin(hds) | F.dayofweek(F.col(\"Date\")).isin([1, 7]), 1).otherwise(0),\n",
    "  \"isOvernight\": F.when(F.col(\"hour\").isin(list(range(20, 24))+list(range(0, 6))), 1).otherwise(0)}) \\\n",
    "  .withColumn(\"isRushhour\", \n",
    "              F.when(F.col(\"hour\").isin(list(range(16, 20))) | (F.col(\"isWeekend\") == 0), 1).otherwise(0)\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ac26fa1-84ed-4e04-8618-60fff6dc2481",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=df.withColumn(\"trip_meters\",F.col('trip_miles')*1609.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4190e0e9-3f0b-4e73-86bb-f70eb9957015",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abafd7e7-e181-4ecd-8ed5-b0a30e2f4c42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.filter(df.year==2024)\n",
    "df = df.drop(*['Date','trip_miles','year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "823f24ae-c3ca-47ba-8700-3ced0e0953ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df =df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06285f01-ed24-4bc7-b075-0912063647a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "# Set the random seed (for reproducibility)\n",
    "seed = 42\n",
    "# Split the DataFrame into training and test sets using a fixed random split\n",
    "train_df, test_df = df.randomSplit([0.7, 0.3], seed=seed)\n",
    "# Print the sizes of the training and test sets\n",
    "print(f\"Training Data Size: {train_df.count()}\")\n",
    "print(f\"Test Data Size: {test_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "866e6d27-a1d0-4a38-9431-d4eec6aabe8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\",labelCol='base_passenger_fare')\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "123a82e2-694f-4e01-b2d5-f051af691418",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vectorAssembler3 = VectorAssembler(\n",
    "    inputCols=['trip_time', 'day', 'isWeekend', 'isOvernight', 'isRushhour', 'trip_meters'],\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26633583-7a65-4829-84da-c798cf3d9f79",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#vectorAssembler3.transform(train_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1534b2f8-a52e-45c9-8e36-eda73dfc4fe8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "lr = GBTRegressor(maxDepth=5,labelCol=\"base_passenger_fare\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81964944-3d04-43f7-9b8d-73e1c2b7b51a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "lr_pipeline = Pipeline(stages=[vectorAssembler3, lr])\n",
    "lr_pipeline_model = lr_pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af32d119-6d2b-40f4-82d2-f7f4941fd2b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lr_predictions = lr_pipeline_model.transform(test_df)\n",
    "rmse = evaluator.evaluate(lr_predictions)\n",
    "print(f\"The Root Mean Squared Error: {rmse}\")\n",
    "end = time.time()\n",
    "print(f\"Time Required to run the model: {end-start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d1cce6f-6fb8-4a9a-b2f0-1dd55e0ec399",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#lr_pipeline_model.transform(test_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64fb5a98-4c06-436e-90cd-a79378a1102e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "partitions = [1,2,4,8,16,32,64,128,256,512]\n",
    "for i in partitions:\n",
    "  start_time = time.time()\n",
    "  train_df_repartition= train_df.repartition(i).cache()\n",
    "  lr_pipeline_model = lr_pipeline.fit(train_df)\n",
    "  lr_predictions = lr_pipeline_model.transform(test_df)\n",
    "  end_time= time.time()\n",
    "  with open(\"Global_Ensemble_gbt.csv\", \"a\") as f:\n",
    "        print(\n",
    "              f\"{i},{evaluator.evaluate(lr_predictions)},{end - start}\",\n",
    "              file=f,\n",
    "              )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Global_Ensemble",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
