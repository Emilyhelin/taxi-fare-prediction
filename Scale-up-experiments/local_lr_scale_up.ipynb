{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad27a295-2824-414e-9147-c21a36992d6a",
     "showTitle": false,
     "title": ""
    },
    "id": "_5pP76M0Ny-b"
   },
   "source": [
    "### Scale-up experiment on Feb 2024 Yellow Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bcc3d63-7914-4146-919c-efc501af7985",
     "showTitle": false,
     "title": ""
    },
    "id": "IazjNdy-Ny-e"
   },
   "outputs": [],
   "source": [
    "cores = [1, 2, 4, 8, 10] # for scale up\n",
    "sizes = [10, 20, 40, 80, 100]\n",
    "directory = 'dbfs:/mnt/2024-team14/yellow_taxi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2b6bf1d-592f-4b2a-9df9-91b6c68457c6",
     "showTitle": false,
     "title": ""
    },
    "id": "IzTOYIQaNy-d"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Row\n",
    "\n",
    "from pyspark.sql.types import DoubleType, DateType\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9827f882-16bd-452f-9785-c9ecfadc9d81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# cols to rename and reformat\n",
    "cols = [\n",
    "    (\"tpep_pickup_datetime\", \"Date\", DateType()),\n",
    "    (\"trip_distance\", \"trip_miles\", DoubleType()),\n",
    "    (\"fare_amount\", \"base_passenger_fare\", DoubleType())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85bd7350-cdf5-4883-9341-bc813194858b",
     "showTitle": false,
     "title": ""
    },
    "id": "xcCfJRO3Ny-e"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import time\n",
    "\n",
    "def build_model(partition_data_iter):\n",
    "  column_names = ['trip_km', 'trip_time', 'base_passenger_fare']\n",
    "  \n",
    "  start = time.time()\n",
    "\n",
    "  partition_data_df = pd.DataFrame(partition_data_iter, columns=column_names)\n",
    "  reg = LinearRegression()\n",
    "  X_train = partition_data_df[['trip_km', 'trip_time']]\n",
    "  y_train = partition_data_df[\"base_passenger_fare\"]\n",
    "  model = reg.fit(X_train.values,y_train.values)\n",
    "\n",
    "  end = time.time()\n",
    "  return [(model, end - start)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7443530-1096-404a-9479-1714bc277201",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def predict(instance):\n",
    "  return[m.predict([instance[:-1]])[0] for m in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f205b077-7912-4c90-b713-ce4c97acaa1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def agg_predictions(preds):\n",
    "  mean = sum(preds) / len(preds)\n",
    "  return float(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1268081-c002-43a7-a6ae-39b10449a5e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def transform(instance):\n",
    "  return Row(**instance.asDict(), \\\n",
    "             raw_prediction = agg_predictions(predict(instance)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcd3c3db-a8a8-4edf-9fff-a8c8190a391b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for _ in range(10): # perform experiments 10 times\n",
    "  for i, c in enumerate(cores):\n",
    "    fraction = sizes[i]\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(f\"local[{c}]\") \\\n",
    "        .appName(\"Local LR with {c} partitions\") \\\n",
    "        .config(\"spark.sql.parquet.enableVectorizedReader\", \"false\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # swith the latest spark version to older one so that it tolerates some data format issues\n",
    "    spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "    \"\"\"\n",
    "    in order to avoid \"Parquet column cannot be converted\" error, we need to disable vectorized reader when we have decimal values in our columns.\n",
    "    refer to https://learn.microsoft.com/en-us/answers/questions/853861/parquet-column-cannot-be-converted for further info\n",
    "    \"\"\"\n",
    "    # spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\", \"false\")\n",
    "\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    # Read the Parquet file with schema inference\n",
    "    df = spark.read.parquet(directory) \\\n",
    "    .select(\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"trip_distance\", \"fare_amount\")\n",
    "\n",
    "    df = df.withColumns({n: F.col(o).cast(t) for o, n, t in cols}) \\\n",
    "    .withColumns({\n",
    "      \"trip_time\": F.unix_timestamp(F.col(\"tpep_dropoff_datetime\")) - F.unix_timestamp(F.col(\"tpep_pickup_datetime\")),\n",
    "      \"trip_km\": F.col(\"trip_miles\") * 0.621371\n",
    "      }).drop(*[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"trip_distance\", \"trip_miles\", \"Date\", \"fare_amount\"])\n",
    "    \n",
    "    df.cache()\n",
    "\n",
    "    sample_df = df.sample(fraction/100)\n",
    "\n",
    "    # split data into train and test\n",
    "    train, test = sample_df.randomSplit([0.7, 0.3], seed=555)\n",
    "    \n",
    "    train_rdd = train.rdd.repartition(c).cache()\n",
    "    test_rdd = test.rdd.repartition(c)\n",
    "\n",
    "    train_rdd.count()\n",
    "    \n",
    "    start = time.time()\n",
    "    models_runtimes = train_rdd.mapPartitions(build_model).collect()\n",
    "    models, runtimes = zip(*models_runtimes)\n",
    "    end = time.time()\n",
    "\n",
    "    with open(\"scaleup.csv\", \"a\") as f:\n",
    "        print(\n",
    "            f\"{c},{fraction},{end - start},{sum(runtimes)/len(runtimes)}\",\n",
    "            file=f,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "local_lr_scale_up",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
