{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad27a295-2824-414e-9147-c21a36992d6a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Exploratory Data Analysis and Creating a dataframe for heat map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da795523-13d4-40a1-9a3b-c55383ac5301",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Feature engineering and correlation matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ea71d9a-450b-4214-b119-81fe7c0901ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(sc, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a184783-3aa6-46bf-929c-ae8d42324b44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"MLlib lab\") \\\n",
    "    .config(\"spark.sql.parquet.enableVectorizedReader\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "# swith the latest spark version to older one so that it tolerates some data format issues\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "\"\"\" \n",
    "in order to avoid \"Parquet column cannot be converted\" error, we need to disable vectorized reader when we have decimal values in our columns. \n",
    "refer to https://learn.microsoft.com/en-us/answers/questions/853861/parquet-column-cannot-be-converted for further info\n",
    "\"\"\"\n",
    "spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\", \"false\") \n",
    "\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2b6bf1d-592f-4b2a-9df9-91b6c68457c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Row\n",
    "\n",
    "from pyspark.sql.types import StringType, TimestampNTZType, LongType, DoubleType, IntegerType, DateType\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eab55f8d-dcbb-4662-aaf7-57da22815325",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "year_range = (2019, 2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec98d14f-57ea-41cb-ba97-9ce367e68ef1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We have encountered \"Parquet column cannot be converted\" error. As a workaround we decided to loop through directory and ensure there is no column type mismatch by checking file by file.\n",
    "\"\"\"\n",
    "\n",
    "directory = '/mnt/2024-team14/'\n",
    "\n",
    "mta_df = spark.read.csv(directory + \"csvs/MTA_2020mar_2024apr.csv\", header=True)\n",
    "weather_df = spark.read.csv(directory + \"csvs/weather_data.csv\", header=True)\n",
    "taxi_zone_df = spark.read.csv(directory + 'csvs/taxi_zone_lookup.csv', header=True)\n",
    "\n",
    "# Read the Parquet file with schema inference\n",
    "df = spark.read.parquet(directory)\n",
    "\n",
    "df = df.select(\n",
    "    \"pickup_datetime\",\n",
    "    \"trip_miles\",\n",
    "    \"trip_time\",\n",
    "    \"base_passenger_fare\",\n",
    "    \"PULocationID\"\n",
    "    ) \\\n",
    "    .withColumns({\n",
    "    \"pickupdayofyear\": F.dayofyear(F.col(\"pickup_datetime\")),\n",
    "    \"pickupmonth\": F.month(F.col(\"pickup_datetime\")),\n",
    "    \"pickupyear\": F.year(F.col(\"pickup_datetime\")),\n",
    "    \"pickupdate\": F.col(\"pickup_datetime\").cast(DateType()),\n",
    "    \"pickuphour\": F.hour(\"pickup_datetime\"),\n",
    "    \"speed\": F.col(\"trip_miles\") / F.col(\"trip_time\") * 3600,\n",
    "    \"week\": F.weekofyear(\"pickup_datetime\") + (F.year(\"pickup_datetime\") - 2019) * 52 - 4\n",
    "    }) \\\n",
    "    .drop(\"pickup_datetime\")\n",
    "\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c16aa622-cf25-4e86-beef-c226421d659a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "total_rows = df.count()\n",
    "df.cache()\n",
    "print(f\"Total number of rows:{total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f66f7ac-efc2-41c9-b9d1-9901f8682e45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df0 = df.groupBy(\"pickupdate\", \"pickupyear\", \"pickupmonth\", \"pickuphour\") \\\n",
    "  .agg(\n",
    "    F.count(\"*\").alias(\"hourly_numTrips\"),\n",
    "    F.sum(\"trip_miles\").alias(\"hourly_total_miles\"), \n",
    "    F.sum(\"trip_time\").alias(\"hourly_total_time\"), \n",
    "    F.sum(\"base_passenger_fare\").alias(\"hourly_total_base_fare\"),\n",
    "    F.mean(\"speed\").alias(\"hourly_mean_speed\")\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ce2c8eb-7db5-4e93-8f62-de10cb28c517",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df0 = df0.join(df, [\"pickupdate\", \"pickupyear\", \"pickupmonth\", \"pickuphour\"])\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7ee0b1e-cf11-453a-97e6-ce6317f594c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = df.groupBy(\"pickupdate\", \"pickupyear\", \"pickupmonth\", \"pickupdayofyear\") \\\n",
    "  .agg(\n",
    "    F.count(\"*\").alias(\"daily_numTrips\"), \n",
    "    F.sum(\"trip_miles\").alias(\"daily_total_miles\"), \n",
    "    F.sum(\"trip_time\").alias(\"daily_total_time\"), \n",
    "    F.sum(\"base_passenger_fare\").alias(\"daily_total_base_fare\"),\n",
    "    F.mean(\"speed\").alias(\"daily_mean_speed\")\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c40131f-8aa5-438b-a8c1-c116475e15fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = df1.join(df0, [\"pickupdate\", \"pickupyear\", \"pickupmonth\", \"pickupdayofyear\"])\n",
    "df1.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d1763d8-1ece-46ab-835b-032b5f6531d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import holidays\n",
    "\n",
    "hds = []\n",
    "for y in range(year_range[0], year_range[1]):\n",
    "  hds += holidays.US(state=\"NY\", years=y).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "951002ca-f8e3-495a-bce9-0b0e065dd9c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2 = df1.withColumns({\n",
    "  \"isWeekend\": F.when(F.col(\"pickupdate\").isin(hds) | F.dayofweek(F.col(\"pickupdate\")).isin([1, 7]), 1).otherwise(0),\n",
    "  \"isOvernight\": F.when(F.col(\"pickuphour\").isin(list(range(20, 24))+list(range(0, 6))), 1).otherwise(0),\n",
    "  \"d_base_fare_per_mile\": F.col(\"daily_total_base_fare\") / F.col(\"daily_total_miles\"),\n",
    "  \"d_base_fare_per_min\": F.col(\"daily_total_base_fare\") / F.col(\"daily_total_time\") * 60,\n",
    "  \"h_base_fare_per_mile\": F.col(\"hourly_total_base_fare\") / F.col(\"hourly_total_miles\"),\n",
    "  \"h_base_fare_per_min\": F.col(\"hourly_total_base_fare\") / F.col(\"hourly_total_time\") * 60\n",
    "  }) \\\n",
    "  .withColumn(\"isRushhour\", \n",
    "              F.when(F.col(\"pickuphour\").isin(list(range(16, 20))) | (F.col(\"isWeekend\") == 0), 1).otherwise(0)\n",
    "              )\n",
    "df2.cache()\n",
    "df1.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b288c31-faa9-42c3-83db-7c58eb2fe356",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Incorporating NY Daily Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff08f211-07c0-4c85-bc25-2ec1183d20e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weather_df = weather_df.drop(*[\"PRCP (Inches)\", \"SNOW (Inches)\", \"SNWD (Inches)\"]) \\\n",
    "  .withColumns({\n",
    "    \"Date\": F.to_date(F.col(\"Date\"), \"dd-MM-yyyy\"),\n",
    "    \"TMAX (Degrees Fahrenheit)\": F.col(\"TMAX (Degrees Fahrenheit)\").cast(DoubleType()),\n",
    "    \"TMIN (Degrees Fahrenheit)\": F.col(\"TMIN (Degrees Fahrenheit)\").cast(DoubleType()),\n",
    "    \"PRCP (mm)\": F.col(\"PRCP (mm)\").cast(DoubleType()),\n",
    "    \"SNOW (mm)\": F.col(\"SNOW (mm)\").cast(DoubleType()),\n",
    "    \"SNWD (mm)\": F.col(\"SNWD (mm)\").cast(DoubleType())\n",
    "  })\n",
    "\n",
    "display(weather_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47ff9bd2-bf18-412f-bc7b-a1ccb5539571",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weather_df=weather_df.withColumns({\n",
    "  \"pickupdayofyear\": F.dayofyear(F.col('Date')),\n",
    "  \"pickupmonth\": F.month(F.col('Date')),\n",
    "  \"pickupyear\": F.year(F.col('Date'))})\n",
    "\n",
    "display(weather_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17b9a72a-d2b5-44f2-8a8a-265c5f2fd75c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weather_df = weather_df \\\n",
    "  .drop(F.col(\"Date\"))\n",
    "\n",
    "display(weather_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9118bc1-4a03-47a1-91ef-e5c8afcb608c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_df = df2.join(weather_df, [\"pickupyear\", \"pickupmonth\", \"pickupdayofyear\"]).drop(\"pickupdate\")\n",
    "\n",
    "joined_df.cache()\n",
    "df2.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4cd8760-89f1-4f07-a96d-c53bbf6f968f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler(\n",
    "    inputCols=joined_df.columns,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"\n",
    "    )\n",
    "feature_vectors = vectorAssembler.transform(joined_df).select(\"features\")\n",
    "\n",
    "# create a correlation matrix\n",
    "correlation = Correlation.corr(feature_vectors, \"features\").collect()[0][0]\n",
    "corr_matrix = correlation.toArray().tolist()\n",
    "corr_matrix_df = pd.DataFrame(data=corr_matrix, columns=joined_df.columns, index=joined_df.columns)\n",
    "corr_matrix_df.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "836805cc-734f-4395-9548-6be56cf2eac3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create feature for extremity of the temperture (abs(temp-68) 20C is 68F)\n",
    "joined_df1 = joined_df.withColumns({\n",
    "  \"TMAXExtremity\": F.abs(F.col(\"TMAX (Degrees Fahrenheit)\") - 68), \n",
    "  \"TMINExtremity\": F.abs(F.col(\"TMIN (Degrees Fahrenheit)\") - 68)\n",
    "  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45c00a1c-5cce-41b0-a3ce-c2b2871b5a4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler(\n",
    "    inputCols=joined_df1.columns,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"\n",
    "    )\n",
    "feature_vectors = vectorAssembler.transform(joined_df1).select(\"features\")\n",
    "\n",
    "# create a correlation matrix\n",
    "correlation = Correlation.corr(feature_vectors, \"features\").collect()[0][0]\n",
    "corr_matrix = correlation.toArray().tolist()\n",
    "corr_matrix_df = pd.DataFrame(data=corr_matrix, columns=joined_df1.columns, index=joined_df1.columns)\n",
    "corr_matrix_df.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01e171af-fd13-463c-9396-59f73762e1ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Incorporating MTA Public Transport Daily Ridership Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e06d02f-cf8a-47fe-a892-6b7d597b2364",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mta_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc6a68ea-a3e7-46ec-9888-02a5968798fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mta_df = mta_df.drop(*[\n",
    "    'Subways: % of Comparable Pre-Pandemic Day',\n",
    "    'Buses: % of Comparable Pre-Pandemic Day',\n",
    "    'LIRR: % of Comparable Pre-Pandemic Day',\n",
    "    'Metro-North: % of Comparable Pre-Pandemic Day',\n",
    "    'Access-A-Ride: % of Comparable Pre-Pandemic Day',\n",
    "    'Bridges and Tunnels: % of Comparable Pre-Pandemic Day',\n",
    "    'Staten Island Railway: % of Comparable Pre-Pandemic Day'\n",
    "  ]) \\\n",
    "  .withColumn(\"Date\", F.to_date(F.col('Date'), \"MM/dd/yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f7dd04d-b32d-442f-810a-c64c52bbab9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mta_df = mta_df.withColumnsRenamed({\n",
    "  \"Subways: Total Estimated Ridership\": \"subways_daily\",\n",
    "  \"Buses: Total Estimated Ridership\": 'buses_daily',\n",
    "  \"LIRR: Total Estimated Ridership\": 'LIRR_daily',\n",
    "  \"Metro-North: Total Estimated Ridership\": 'metro_north_daily',\n",
    "  \"Access-A-Ride: Total Scheduled Trips\": 'Access-A-Ride_daily',\n",
    "  \"Bridges and Tunnels: Total Traffic\": 'br_tunnel_traffic',\n",
    "  \"Staten Island Railway: Total Estimated Ridership\": 'SIR_daily'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f6865ec-4221-42ac-9abb-24b66a89b788",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mta_df = mta_df.withColumns({\n",
    "    \"pickupdayofyear\": F.dayofyear(F.col('Date')),\n",
    "    \"pickupmonth\": F.month(F.col('Date')),\n",
    "    \"pickupyear\": F.year(F.col('Date')),\n",
    "    'subways_daily': F.col('subways_daily').cast(\"int\"),\n",
    "    'buses_daily': F.col('buses_daily').cast(\"int\"),\n",
    "    'LIRR_daily': F.col('LIRR_daily').cast(\"int\"),\n",
    "    'metro_north_daily': F.col('metro_north_daily').cast(\"int\"),\n",
    "    'Access-A-Ride_daily': F.col('Access-A-Ride_daily').cast(\"int\"),\n",
    "    'br_tunnel_traffic': F.col('br_tunnel_traffic').cast(\"int\"),\n",
    "    'SIR_daily': F.col('SIR_daily').cast(\"int\")\n",
    "  }) \\\n",
    "  .drop(F.col(\"Date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2482c110-132f-4395-a0e0-7b08ac07be87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(mta_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f2c2782-a3f3-450d-85d5-921a258148d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mta_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "315ef98c-2b78-417f-a397-e227f071d8f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_df2 = joined_df1.join(mta_df, [\"pickupyear\", \"pickupmonth\", \"pickupdayofyear\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "420fb990-6678-43f8-aed1-ffd3f7cb7ce1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_df2.cache()\n",
    "joined_df1.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd9fb538-d802-44cb-a7d6-b3337200733f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler(\n",
    "    inputCols=joined_df2.columns,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"\n",
    "    )\n",
    "feature_vectors = vectorAssembler.transform(joined_df2).select(\"features\")\n",
    "\n",
    "# create a correlation matrix\n",
    "correlation = Correlation.corr(feature_vectors, \"features\").collect()[0][0]\n",
    "corr_matrix = correlation.toArray().tolist()\n",
    "corr_matrix_df = pd.DataFrame(data=corr_matrix, columns=joined_df2.columns, index=joined_df2.columns)\n",
    "corr_matrix_df.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc8c2205-fb1b-478f-8882-09df7efaf9c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Creating pt_taxi_ratio which represents the ratio of the number of public transport users and taxi passengers in each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef3fe5b5-da4f-4483-8969-395d29a1f8cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# add an additional feature of taxi trips / public transport ridership (daily)\n",
    "joined_df3 = joined_df2.withColumn(\"pt_taxi_ratio\", \n",
    "        (F.col(\"subways_daily\") + F.col(\"buses_daily\") + F.col(\"LIRR_daily\") + F.col(\"metro_north_daily\") + F.col(\"Access-A-Ride_daily\") + F.col(\"SIR_daily\")) / F.col(\"daily_numTrips\")\n",
    "    )\n",
    "joined_df3.cache()\n",
    "joined_df2.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff12e8af-6df2-4096-a8fc-4084c8c0e495",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler(\n",
    "    inputCols=joined_df3.columns,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"\n",
    "    )\n",
    "feature_vectors = vectorAssembler.transform(joined_df3).select(\"features\")\n",
    "\n",
    "# create a correlation matrix\n",
    "correlation = Correlation.corr(feature_vectors, \"features\").collect()[0][0]\n",
    "corr_matrix = correlation.toArray().tolist()\n",
    "corr_matrix_df = pd.DataFrame(data=corr_matrix, columns=joined_df3.columns, index=joined_df3.columns)\n",
    "corr_matrix_df.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52837827-6a6a-40ec-9058-10986c08af5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_df4 = joined_df3.drop(\n",
    "  \"subways_daily\",\n",
    "  \"buses_daily\",\n",
    "  \"LIRR_daily\",\n",
    "  \"metro_north_daily\", \n",
    "  \"Access-A-Ride_daily\", \n",
    "  \"SIR_daily\", \n",
    "  \"daily_numTrips\", \n",
    "  \"daily_total_miles\", \n",
    "  \"daily_total_time\", \n",
    "  \"daily_total_base_fare\",\n",
    "  \"daily_mean_speed\",\n",
    "  \"hourly_numTrips\",\n",
    "  \"hourly_total_miles\",\n",
    "  \"hourly_total_time\",\n",
    "  \"hourly_total_base_fare\",\n",
    "  \"hourly_mean_speed\",\n",
    "  \"TMAX (Degrees Fahrenheit)\",\n",
    "  \"TMIN (Degrees Fahrenheit)\",\n",
    "  \"d_base_fare_per_mile\",\n",
    "  \"d_base_fare_per_min\",\n",
    "  \"h_base_fare_per_mile\",\n",
    "  \"h_base_fare_per_min\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ed6c44f-412b-4447-8bb5-2a1fc1aae464",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_df5 = df.join(joined_df4, [\"pickupyear\", \"pickupmonth\", \"pickupdayofyear\", \"pickuphour\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74b0a2d6-8166-444f-bf0d-79de3090c37e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df6 = joined_df5\n",
    "df6.cache()\n",
    "joined_df3.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc7dce2b-672d-4418-9f20-8fb489818de7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler(\n",
    "    inputCols=df6.columns,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"\n",
    "    )\n",
    "feature_vectors = vectorAssembler.transform(df6).select(\"features\")\n",
    "\n",
    "# create a correlation matrix\n",
    "correlation = Correlation.corr(feature_vectors, \"features\").collect()[0][0]\n",
    "corr_matrix = correlation.toArray().tolist()\n",
    "corr_matrix_df = pd.DataFrame(data=corr_matrix, columns=df6.columns, index=df6.columns)\n",
    "corr_matrix_df.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fba0b82-62b9-4200-8d6f-cf97b85e61f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Dataframe for creating a heat map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f28f591-8e3c-4d80-a96e-205b994a98c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "taxi_zone_df = taxi_zone_df.drop(\"service_zone\")\n",
    "taxi_zone_df = taxi_zone_df.withColumns(\n",
    "  {\n",
    "    \"PULocationID\": F.col(\"LocationID\"),\n",
    "    \"PUBorough\": F.col(\"Borough\"),\n",
    "    \"PUZone\": F.col(\"Zone\"),\n",
    "    \"DOLocationID\": F.col(\"LocationID\"),\n",
    "    \"DOBorough\": F.col(\"Borough\"),\n",
    "    \"DOZone\": F.col(\"Zone\")\n",
    "  }) \\\n",
    "  .drop(\"LocationID\", \"Borough\", \"Zone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2252aede-b169-4fac-bd67-148681f12b0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_for_count = df.select(\"pickup_datetime\", \"PULocationID\") \\\n",
    "  .withColumns({\n",
    "    \"pickupdate\": F.col(\"pickup_datetime\").cast(DateType()),\n",
    "    \"pickuphour\": F.hour(F.col(\"pickup_datetime\"))}) \\\n",
    "  .withColumns({\n",
    "    \"isWeekend\": F.when(F.col(\"pickupdate\").isin(hds) | F.dayofweek(F.col(\"pickupdate\")).isin([1, 7]), 1).otherwise(0),\n",
    "    \"isOvernight\": F.when(F.col(\"pickuphour\").isin(list(range(20, 24))+list(range(0, 6))), 1).otherwise(0)}) \\\n",
    "  .withColumn(\"isRushhour\", \n",
    "              F.when(F.col(\"pickuphour\").isin(list(range(16, 20))) | (F.col(\"isWeekend\") == 0), 1)\n",
    "              .otherwise(0)\n",
    "              ) \\\n",
    "  .join(taxi_zone_df.select(\"PULocationID\", \"PUZone\"), \"PULocationID\") \\\n",
    "  .drop(\"PULocationID\", \"pickup_datetime\", \"pickupdate\", \"pickuphour\")\n",
    "\n",
    "df_for_count.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0df11c7a-6bbf-4422-afad-f468888c2932",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df = df_for_count.groupBy(\"PUZone\") \\\n",
    "  .agg(\n",
    "    F.sum(F.col(\"isOvernight\")).alias(\"overnight_count\"),\n",
    "    F.sum(F.col(\"isRushhour\")).alias(\"rushhour_count\"),\n",
    "    F.sum(F.col(\"isWeekend\")).alias(\"weekend_count\")\n",
    "  )\n",
    "\n",
    "display(result_df) # save the table for future use (e.g., creating heatmap)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "eda_heatmap",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
