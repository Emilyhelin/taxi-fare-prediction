{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad27a295-2824-414e-9147-c21a36992d6a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da795523-13d4-40a1-9a3b-c55383ac5301",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ea71d9a-450b-4214-b119-81fe7c0901ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(sc, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a184783-3aa6-46bf-929c-ae8d42324b44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"MLlib lab\") \\\n",
    "    .config(\"spark.sql.parquet.enableVectorizedReader\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "# swith the latest spark version to older one so that it tolerates some data format issues\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "\"\"\" \n",
    "in order to avoid \"Parquet column cannot be converted\" error, we need to disable vectorized reader when we have decimal values in our columns. \n",
    "refer to https://learn.microsoft.com/en-us/answers/questions/853861/parquet-column-cannot-be-converted for further info\n",
    "\"\"\"\n",
    "spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\", \"false\") \n",
    "\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2b6bf1d-592f-4b2a-9df9-91b6c68457c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Row\n",
    "\n",
    "from pyspark.sql.types import StringType, TimestampNTZType, LongType, DoubleType, IntegerType, DateType\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eab55f8d-dcbb-4662-aaf7-57da22815325",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "year_range = (2019, 2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec98d14f-57ea-41cb-ba97-9ce367e68ef1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We have encountered \"Parquet column cannot be converted\" error. As a workaround we decided to loop through directory and ensure there is no column type mismatch by checking file by file.\n",
    "\"\"\"\n",
    "\n",
    "directory = '/mnt/2024-team14/'\n",
    "\n",
    "# Read the Parquet file with schema inference\n",
    "df = spark.read.parquet(directory)\n",
    "\n",
    "cols_to_drop_first = [\n",
    "    \"hvfhs_license_num\", \n",
    "    \"dispatching_base_num\", \n",
    "    \"originating_base_num\",\n",
    "    \"request_datetime\",\n",
    "    \"on_scene_datetime\",\n",
    "    \"dropoff_datetime\",\n",
    "    \"shared_request_flag\",\n",
    "    \"shared_match_flag\",\n",
    "    \"access_a_ride_flag\",\n",
    "    \"wav_request_flag\",\n",
    "    \"wav_match_flag\"\n",
    "]\n",
    "\n",
    "# (column with mismatch, desirable type)\n",
    "mismatch_col = [\n",
    "    (\"airport_fee\", \"double\"), \n",
    "    (\"PULocationID\", \"bigint\"), \n",
    "    (\"DOLocationID\", \"bigint\")\n",
    "]\n",
    "\n",
    "df = df.drop(*cols_to_drop_first) \\\n",
    "    .withColumns({c: F.col(c).cast(t) for c, t in mismatch_col}) \\\n",
    "    .withColumns({\n",
    "    \"trip_meters\": F.col('trip_miles')*1609.35,\n",
    "    \"pickupdayofyear\": F.dayofyear(F.col(\"pickup_datetime\")),\n",
    "    \"pickupmonth\": F.month(F.col(\"pickup_datetime\")),\n",
    "    \"pickupyear\": F.year(F.col(\"pickup_datetime\")),\n",
    "    \"pickupdate\": F.col(\"pickup_datetime\").cast(DateType()),\n",
    "    \"pickuphour\": F.hour(\"pickup_datetime\"),\n",
    "    \"week\": F.weekofyear(\"pickup_datetime\") + (F.year(\"pickup_datetime\") - 2019) * 52 - 4\n",
    "    }) \\\n",
    "    .drop(*[\"pickup_datetime\", \"trip_miles\"])\n",
    "\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c16aa622-cf25-4e86-beef-c226421d659a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "total_rows = df.count()\n",
    "df.cache()\n",
    "print(f\"Total number of rows:{total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4bd9563-0f11-4cf8-8d7e-329ae2ba7915",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = df.groupBy(\"pickupdate\") \\\n",
    "  .agg(\n",
    "    F.count(\"*\").alias(\"daily_numTrips\")\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90ebc390-7a88-4eba-be78-6629e8d232d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.join(df1, [\"pickupdate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d1763d8-1ece-46ab-835b-032b5f6531d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import holidays\n",
    "\n",
    "hds = []\n",
    "for y in range(year_range[0], year_range[1]):\n",
    "  hds += holidays.US(state=\"NY\", years=y).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "951002ca-f8e3-495a-bce9-0b0e065dd9c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2 = df.withColumns({\n",
    "  \"isWeekend\": F.when(F.col(\"pickupdate\").isin(hds) | F.dayofweek(F.col(\"pickupdate\")).isin([1, 7]), 1).otherwise(0),\n",
    "  \"isOvernight\": F.when(F.col(\"pickuphour\").isin(list(range(20, 24))+list(range(0, 6))), 1).otherwise(0)\n",
    "  }) \\\n",
    "  .withColumn(\"isRushhour\", \n",
    "              F.when(F.col(\"pickuphour\").isin(list(range(16, 20))) | (F.col(\"isWeekend\") == 0), 1).otherwise(0)\n",
    "              )\n",
    "df2.cache()\n",
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b288c31-faa9-42c3-83db-7c58eb2fe356",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Incorporating NY Daily Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6daf2efe-57b3-49a8-a921-1fe05a3264aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weather_df = spark.read.csv(directory + \"csvs/weather_data.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff08f211-07c0-4c85-bc25-2ec1183d20e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weather_df = weather_df.drop(*[\"PRCP (Inches)\", \"SNOW (Inches)\", \"SNWD (Inches)\"]) \\\n",
    "  .withColumns({\n",
    "    \"Date\": F.to_date(F.col(\"Date\"), \"dd-MM-yyyy\"),\n",
    "    \"TMAX (Degrees Fahrenheit)\": F.col(\"TMAX (Degrees Fahrenheit)\").cast(DoubleType()),\n",
    "    \"TMIN (Degrees Fahrenheit)\": F.col(\"TMIN (Degrees Fahrenheit)\").cast(DoubleType()),\n",
    "    \"PRCP (mm)\": F.col(\"PRCP (mm)\").cast(DoubleType()),\n",
    "    \"SNOW (mm)\": F.col(\"SNOW (mm)\").cast(DoubleType()),\n",
    "    \"SNWD (mm)\": F.col(\"SNWD (mm)\").cast(DoubleType())\n",
    "  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47ff9bd2-bf18-412f-bc7b-a1ccb5539571",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weather_df=weather_df.withColumns({\n",
    "  \"pickupdayofyear\": F.dayofyear(F.col('Date')),\n",
    "  \"pickupmonth\": F.month(F.col('Date')),\n",
    "  \"pickupyear\": F.year(F.col('Date'))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11976d96-5800-47f7-8b73-c5ca38310604",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weather_df = weather_df \\\n",
    "  .drop(F.col(\"Date\"))\n",
    "\n",
    "display(weather_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9118bc1-4a03-47a1-91ef-e5c8afcb608c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_df = df2.join(weather_df, [\"pickupyear\", \"pickupmonth\", \"pickupdayofyear\"]).drop(\"pickupdate\")\n",
    "\n",
    "joined_df.cache()\n",
    "df2.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "836805cc-734f-4395-9548-6be56cf2eac3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create feature for extremity of the temperture (abs(temp-68) 20C is 68F)\n",
    "joined_df1 = joined_df.withColumns({\n",
    "  \"TMAXExtremity\": F.abs(F.col(\"TMAX (Degrees Fahrenheit)\") - 68), \n",
    "  \"TMINExtremity\": F.abs(F.col(\"TMIN (Degrees Fahrenheit)\") - 68)\n",
    "  })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01e171af-fd13-463c-9396-59f73762e1ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Incorporating MTA Public Transport Daily Ridership Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46abaa76-3f07-4da5-b768-27934e47aa6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mta_df = spark.read.csv(directory + \"csvs/MTA_2020mar_2024apr.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e06d02f-cf8a-47fe-a892-6b7d597b2364",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mta_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc6a68ea-a3e7-46ec-9888-02a5968798fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mta_df = mta_df.drop(*[\n",
    "    'Subways: % of Comparable Pre-Pandemic Day',\n",
    "    'Buses: % of Comparable Pre-Pandemic Day',\n",
    "    'LIRR: % of Comparable Pre-Pandemic Day',\n",
    "    'Metro-North: % of Comparable Pre-Pandemic Day',\n",
    "    'Access-A-Ride: % of Comparable Pre-Pandemic Day',\n",
    "    'Bridges and Tunnels: % of Comparable Pre-Pandemic Day',\n",
    "    'Staten Island Railway: % of Comparable Pre-Pandemic Day'\n",
    "  ]) \\\n",
    "  .withColumn(\"Date\", F.to_date(F.col('Date'), \"MM/dd/yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f7dd04d-b32d-442f-810a-c64c52bbab9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mta_df = mta_df.withColumnsRenamed({\n",
    "  \"Subways: Total Estimated Ridership\": \"subways_daily\",\n",
    "  \"Buses: Total Estimated Ridership\": 'buses_daily',\n",
    "  \"LIRR: Total Estimated Ridership\": 'LIRR_daily',\n",
    "  \"Metro-North: Total Estimated Ridership\": 'metro_north_daily',\n",
    "  \"Access-A-Ride: Total Scheduled Trips\": 'Access-A-Ride_daily',\n",
    "  \"Bridges and Tunnels: Total Traffic\": 'br_tunnel_traffic',\n",
    "  \"Staten Island Railway: Total Estimated Ridership\": 'SIR_daily'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f6865ec-4221-42ac-9abb-24b66a89b788",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mta_df = mta_df.withColumns({\n",
    "    \"pickupdayofyear\": F.dayofyear(F.col('Date')),\n",
    "    \"pickupmonth\": F.month(F.col('Date')),\n",
    "    \"pickupyear\": F.year(F.col('Date')),\n",
    "    'subways_daily': F.col('subways_daily').cast(\"int\"),\n",
    "    'buses_daily': F.col('buses_daily').cast(\"int\"),\n",
    "    'LIRR_daily': F.col('LIRR_daily').cast(\"int\"),\n",
    "    'metro_north_daily': F.col('metro_north_daily').cast(\"int\"),\n",
    "    'Access-A-Ride_daily': F.col('Access-A-Ride_daily').cast(\"int\"),\n",
    "    'br_tunnel_traffic': F.col('br_tunnel_traffic').cast(\"int\"),\n",
    "    'SIR_daily': F.col('SIR_daily').cast(\"int\")\n",
    "  }) \\\n",
    "  .drop(F.col(\"Date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f2c2782-a3f3-450d-85d5-921a258148d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mta_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "315ef98c-2b78-417f-a397-e227f071d8f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_df2 = joined_df1.join(mta_df, [\"pickupyear\", \"pickupmonth\", \"pickupdayofyear\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "420fb990-6678-43f8-aed1-ffd3f7cb7ce1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_df2.cache()\n",
    "joined_df1.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc8c2205-fb1b-478f-8882-09df7efaf9c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Creating pt_taxi_ratio which represents the ratio of the number of public transport users and taxi passengers in each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef3fe5b5-da4f-4483-8969-395d29a1f8cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# add an additional feature of public transport ridership (daily) / taxi trips \n",
    "joined_df3 = joined_df2.withColumn(\"pt_taxi_ratio\", \n",
    "        (F.col(\"subways_daily\") + F.col(\"buses_daily\") + F.col(\"LIRR_daily\") + F.col(\"metro_north_daily\") + F.col(\"Access-A-Ride_daily\") + F.col(\"SIR_daily\")) / F.col(\"daily_numTrips\")\n",
    "    )\n",
    "joined_df3.cache()\n",
    "joined_df2.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52837827-6a6a-40ec-9058-10986c08af5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_df4 = joined_df3.drop(\n",
    "  \"subways_daily\",\n",
    "  \"buses_daily\",\n",
    "  \"LIRR_daily\",\n",
    "  \"metro_north_daily\", \n",
    "  \"Access-A-Ride_daily\", \n",
    "  \"SIR_daily\", \n",
    "  \"TMAX (Degrees Fahrenheit)\",\n",
    "  \"TMIN (Degrees Fahrenheit)\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ed6c44f-412b-4447-8bb5-2a1fc1aae464",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_df5 = df.join(joined_df4, [\"pickupyear\", \"pickupmonth\", \"pickupdayofyear\", \"pickuphour\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74b0a2d6-8166-444f-bf0d-79de3090c37e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df6 = joined_df5.drop(\"pickupdate\")\n",
    "df6.cache()\n",
    "joined_df3.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfa0f8a1-aa1b-4186-98ec-261244302384",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_rdd = df6.rdd.repartition(24).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "578998de-707b-40f2-ad1d-4ebd7e0f93b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "column_names = df6.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ceef337-d2fa-4a84-9b56-2bd03251f5d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_cols = column_names\n",
    "feature_cols.remove(\"base_passenger_fare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4797305c-85ed-4559-8fcc-60ef39844011",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "def build_model(partition_data_it):\n",
    "  partition_data_df = pd.DataFrame(partition_data_it,columns=column_names)\n",
    "  X_train = partition_data_df[feature_cols]\n",
    "  y_train = partition_data_df[\"base_passenger_fare\"]\n",
    "  sel = SelectFromModel(RandomForestRegressor(max_depth= 5, n_estimators = 100))\n",
    "  model = sel.fit(X_train.values,y_train.values)\n",
    "  selected_feat= X_train.columns[(sel.get_support())]\n",
    "  return [selected_feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4613372-8e60-4abe-afcb-7d7541e39494",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "models = df_rdd.mapPartitions(build_model).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "183913da-a5c1-484a-a031-ace115a7d51f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_dict = {\n",
    "    \"pickupyear\": 0,\n",
    "    \"pickupmonth\": 0,\n",
    "    \"pickupdayofyear\": 0,\n",
    "    \"pickuphour\": 0,\n",
    "    \"PULocationID\": 0,\n",
    "    \"DOLocationID\": 0,\n",
    "    \"trip_time\": 0,\n",
    "    \"base_passenger_fare\": 0,\n",
    "    \"tolls\": 0,\n",
    "    \"bcf\": 0,\n",
    "    \"sales_tax\": 0,\n",
    "    \"congestion_surcharge\": 0,\n",
    "    \"airport_fee\": 0,\n",
    "    \"tips\": 0,\n",
    "    \"driver_pay\": 0,\n",
    "    \"trip_meters\": 0,\n",
    "    \"week\": 0,\n",
    "    \"daily_numTrips\": 0,\n",
    "    \"isWeekend\": 0,\n",
    "    \"isOvernight\": 0,\n",
    "    \"isRushhour\": 0,\n",
    "    \"PRCP (mm)\": 0,\n",
    "    \"SNOW (mm)\": 0,\n",
    "    \"SNWD (mm)\": 0,\n",
    "    \"TMAXExtremity\": 0,\n",
    "    \"TMINExtremity\": 0,\n",
    "    \"br_tunnel_traffic\": 0,\n",
    "    \"pt_taxi_ratio\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69299f06-9587-4925-9ec2-de26d3af6dd2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for features in models:\n",
    "  for f in features:\n",
    "    feature_dict[f] += 1\n",
    "\n",
    "feature_dict"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "feature_selection",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
